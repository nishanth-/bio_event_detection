{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to set below two environment variables before invoking this jupyter notebook in python 2.7.\n",
    "#DATAPATH - set to TEES/data folder\n",
    "#TEES_SETTINGS - set to TEES/settings/.tees_local_settings.py\n",
    "\n",
    "import sys, os, types\n",
    "sys.path.append(os.path.dirname(os.curdir))\n",
    "import TEES.Core.SentenceGraph\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATAPATH should be set to the directory containng TEES/data folder.\n",
    "DATAPATH = os.getenv('DATAPATH');\n",
    "CORPUS_DIR = DATAPATH + '/corpora'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = CORPUS_DIR\n",
    "prefix = '/GE11'\n",
    "devfile = folder + prefix +\"-devel.xml\"\n",
    "testfile = folder + prefix +\"-test.xml\"\n",
    "trfile = folder + prefix + \"-train.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentences(input, parse, tokenization, removeNameInfo=False):\n",
    "        if type(input) != types.ListType:\n",
    "            # Load corpus and make sentence graphs\n",
    "            corpusElements = TEES.Core.SentenceGraph.loadCorpus(input, parse, tokenization, removeNameInfo=removeNameInfo)\n",
    "            sentences = []\n",
    "            for sentence in corpusElements.sentences:\n",
    "                if sentence.sentenceGraph != None: # required for event detection\n",
    "                    sentences.append( [sentence.sentenceGraph,None] )\n",
    "            return sentences\n",
    "        else: # assume input is already a list of sentences\n",
    "            assert(removeNameInfo == False)\n",
    "            return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIpData(aSent):\n",
    "    ip_data_list = list();\n",
    "    ip_data_wloc={};\n",
    "    token_list = {}\n",
    "    dep_dict = {};\n",
    "    event_dict = {};\n",
    "    for ent in aSent.entities:\n",
    "        isEvent = ent.get('event');\n",
    "        if isEvent:\n",
    "            event_dict[ent.get('text')] = ent.get('type');\n",
    "    \n",
    "    prev_elem = [];\n",
    "    prev_prev_elem =[];\n",
    "    tok_loc = 0;\n",
    "    total_words = len(aSent.tokens);\n",
    "    for token_elem in aSent.tokens:\n",
    "        ip_data = {'token':'', 'ltoken':'None', 'sentLoc':'', 'sentLocldep':-0.1, 'sentLocrdep':-0.1, 'rtoken':'None', 'ldtoken':'None', 'rdtoken':'None', 'event':'None', 'pos':'', 'ldpos':'None', 'lldpos':'None', 'rdpos':'None', 'rrdpos':'None', 'lcpos':'None', 'llcpos':'None', 'rcpos':'None', 'rrcpos':'None', 'ldep':'None', 'lldep':'None', 'rdep':'None', 'rrdep':'None', 'id':''}\n",
    "        token_list[token_elem.get('id')] = token_elem;\n",
    "        token_val = token_elem.get('text');\n",
    "        ip_data['token'] = token_val;\n",
    "        ip_data['pos'] = token_elem.get('POS');\n",
    "        ip_data['id'] = token_elem.get('id');\n",
    "        ip_data['sentLoc'] = float(tok_loc)/float(total_words);\n",
    "        ip_data_wloc[token_elem.get('id')] = ip_data['sentLoc'];\n",
    "        tok_loc = tok_loc+1;\n",
    "        if token_val in event_dict:\n",
    "            ip_data['event'] = event_dict[token_val];\n",
    "        if prev_elem:\n",
    "            prev_elem['rcpos'] = ip_data['pos'];\n",
    "            ip_data['lcpos'] = prev_elem['pos'];\n",
    "            ip_data['ltoken'] = prev_elem['token'];\n",
    "            prev_elem['rtoken'] = ip_data['token'];\n",
    "        if prev_prev_elem:\n",
    "            prev_prev_elem['rrcpos'] = ip_data['pos'];\n",
    "            ip_data['llcpos'] = prev_prev_elem['pos'];\n",
    "        ip_data_list.append(ip_data);\n",
    "        prev_prev_elem = prev_elem;\n",
    "        prev_elem = ip_data;\n",
    "        \n",
    "        \n",
    "    for dep in aSent.dependencies:\n",
    "        t1_id = dep.get('t1');\n",
    "        t2_id = dep.get('t2');\n",
    "        dtype = dep.get('type');\n",
    "        if t1_id in dep_dict:\n",
    "            dep_obj = dep_dict[t1_id];\n",
    "            dep_obj['rid'] = t2_id;\n",
    "            dep_obj['rdep'] = dtype;\n",
    "        else:\n",
    "            dep_obj = {'lid':'', 'rid':t2_id, 'ldep':'None', 'rdep':dtype}\n",
    "        dep_dict[t1_id] = dep_obj;\n",
    "        \n",
    "        if t2_id in dep_dict:\n",
    "            dep_obj = dep_dict[t2_id];\n",
    "            dep_obj['lid'] = t1_id;\n",
    "            dep_obj['ldep'] = dtype;\n",
    "        else:\n",
    "            dep_obj = {'lid':t1_id, 'rid':'', 'ldep':dtype, 'rdep':'None'}\n",
    "        dep_dict[t2_id] = dep_obj;\n",
    "           \n",
    "        \n",
    "    for ip in ip_data_list:\n",
    "        if ip['id'] not in dep_dict:\n",
    "            continue;\n",
    "        dep_obj = dep_dict[ip['id']];\n",
    "        lid = dep_obj['lid'];\n",
    "        rid = dep_obj['rid'];\n",
    "        if lid:\n",
    "            ip['ldpos'] = token_list[lid].get('POS');\n",
    "            ip['ldtoken'] = token_list[lid].get('text');\n",
    "            ip['sentLocldep'] = ip_data_wloc[lid];\n",
    "            dep_l_obj = dep_dict[lid];\n",
    "            llid = dep_l_obj['lid'];\n",
    "            if llid:\n",
    "                ip['lldpos'] = token_list[llid].get('POS');\n",
    "            ip['lldep'] = dep_l_obj['ldep']\n",
    "        if rid:\n",
    "            ip['rdpos'] = token_list[rid].get('POS');\n",
    "            ip['rdtoken'] = token_list[rid].get('text');\n",
    "            ip['sentLocrdep'] = ip_data_wloc[rid];\n",
    "            dep_r_obj = dep_dict[rid];\n",
    "            rrid = dep_r_obj['rid'];\n",
    "            if rrid:\n",
    "                ip['rrdpos'] = token_list[rrid].get('POS');\n",
    "            ip['rrdep'] = dep_r_obj['rdep'];\n",
    "        ip['ldep'] = dep_obj['ldep'];\n",
    "        ip['rdep'] = dep_obj['rdep'];\n",
    "            \n",
    "        \n",
    "    return ip_data_list;    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToFile(aData, aFile):\n",
    "    with open(aFile, 'wb') as handle:\n",
    "        pickle.dump(aData, handle, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertDataToPickle(aIpFile, aOpFile):\n",
    "    sent_list = getSentences(aIpFile, 'McCC', None)\n",
    "    ip_data = list();\n",
    "    for sent in sent_list:\n",
    "        sent_data = getIpData(sent[0]);\n",
    "        ip_data.append(sent_data);\n",
    "    writeToFile(ip_data, aOpFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading corpus file /Volumes/Study/course_work/ljmu_ml_ai/thesis_work/Code/bio_event_detection/TEES/data/corpora/GE11-train.xml\n",
      "908 documents, 8679 sentences\n",
      "Making sentence graphs (GE11.d1166.s9): 100.00 % (0:0:15.890)                \n",
      "Skipped 2095 duplicate interaction edges in SentenceGraphs\n",
      "Loading corpus file /Volumes/Study/course_work/ljmu_ml_ai/thesis_work/Code/bio_event_detection/TEES/data/corpora/GE11-devel.xml\n",
      "259 documents, 2902 sentences\n",
      "Making sentence graphs (GE11.d258.s5): 100.00 % (0:0:4.786)           \n",
      "Skipped 754 duplicate interaction edges in SentenceGraphs\n",
      "Loading corpus file /Volumes/Study/course_work/ljmu_ml_ai/thesis_work/Code/bio_event_detection/TEES/data/corpora/GE11-test.xml\n",
      "347 documents, 3377 sentences\n",
      "Making sentence graphs (GE11.d1513.s8): 100.00 % (0:0:3.723)         \n",
      "Skipped 0 duplicate interaction edges in SentenceGraphs\n"
     ]
    }
   ],
   "source": [
    "ConvertDataToPickle(trfile, 'processed_data/GE11_tr.pickle')\n",
    "ConvertDataToPickle(devfile, 'processed_data/GE11_dev.pickle')\n",
    "ConvertDataToPickle(testfile, 'processed_data/GE11_test.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
